# -*- coding: utf-8 -*-
"""통계데이터분석-KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19FjXBKDUscoBe27JJy9hMvwc22T_Ic2u
"""

from sklearn.datasets import load_iris

iris_data = load_iris()
iris_data

data = iris_data['data']
target = iris_data['target']
tnames = iris_data['target_names']
for i,y in enumerate(target):
  print(f'{i}:{tnames[y]}')

sepal_lengths = data[:50,[0]] #setosa 품종의 sepal_length만 추출(독립변수로 사용하기 위해 2차원 배열)
sepal_widths = data[:50,1] #setosa 품종의 sepal_width만 추출(종속변수로 사용하기 위해 1차원 배열)

import matplotlib.pyplot as plt
plt.plot(sepal_lengths[:,0],sepal_widths,'ro',label='setosa')
plt.xlabel('sepal-length')
plt.ylabel('sepal-width')
plt.yticks([0,max(sepal_widths)])
plt.title('iris - setosa')
plt.show()

train_xs = sepal_lengths[:len(sepal_lengths)*2//3]
train_ys = sepal_widths[:len(sepal_widths)*2//3]
test_xs = sepal_lengths[len(sepal_lengths)*2//3:]
test_ys = sepal_widths[len(sepal_widths)*2//3:]
print(f"학습용 길이:{len(train_ys)} 테스트용 길이:{len(test_ys)}")

"""거리 계산"""

import numpy as np

def distance(x1,x2):
  if isinstance(x1,int) and isinstance(x2,int): #두 개의 값이 모두 int 형식일 때
    return np.abs(x2-x1) #차이의 절대값을 반환
  if isinstance(x1,list) and isinstance(x2,list):
    x1 = np.array(x1)
    x2 = np.array(x2)
  return sum((x1-x2)**2)**(1/2)

na1 = np.array([1,2])
na2 = np.array([4,6])
distance(na1,na2)

"""k개의 가까운 이웃을 찾아라."""

def find_k_nearest_neighbor(xs,ys,tx,k=5):
  """
  입력 매개변수:xs,ys,tx,k
    xs:독립 변수(학습 데이터)
    ys:종소 변수(학습 데이터)
    tx:독립변수(예측에 사용할 신입)
    k:찾을 이웃 수
  반환: k개의 이웃의 y의 평균값
  """
  sarr=[]
  for i,x in enumerate(xs):
    dis = distance(x,tx)
    sarr.append((dis,i)) #계산한 거리와 인덱스를 보관
    
  sarr.sort(key=lambda x:x[0])#dis 순으로 정렬
  
  k = min(k,len(sarr)) #현재 학습 데이터 개수와 k 중에 최솟값을 k로 확정
  neighbors =[x[1] for x in sarr[:k]] #거리가 가까운 이웃 k개의 인덱스로 리스트 구성
  return sum(ys[neighbors])/k #이웃의 평균 값을 반환

def find_k_nearest_neighbors(xs,ys,t_xs,k=5):
  return [find_k_nearest_neighbor(xs,ys,tx,k) for tx in t_xs]

pred_val = find_k_nearest_neighbors(train_xs,train_ys,test_xs)

plt.plot(pred_val,'ro',label='predict')
plt.plot(test_ys,'b.',label='actual')
plt.ylim(0,5)
plt.legend()
plt.xlabel("index")
plt.ylabel("sepal-width")
plt.title("iris - setosa")
plt.show()

print(np.mean(np.abs(pred_val-test_ys)/test_ys))

from sklearn.neighbors import KNeighborsRegressor

knr_model = KNeighborsRegressor()#모델 개체 생성
knr_model.fit(train_xs,train_ys)#학습하세요.
pred_val2 = knr_model.predict(test_xs)#예측하세요.

plt.plot(pred_val,'ro',label='our function')
plt.plot(pred_val2,'b.',label='KNeighborsRegressor')
plt.ylim(0,5)
plt.legend()
plt.xlabel("index")
plt.ylabel("sepal-width")
plt.title("iris - setosa")
plt.show()

print(np.mean(np.abs(pred_val2-test_ys)/test_ys))

print(find_k_nearest_neighbor(train_xs,train_ys,[3.4]))
print(find_k_nearest_neighbor(train_xs,train_ys,[6]))
print(find_k_nearest_neighbor(train_xs,train_ys,[8]))
print(find_k_nearest_neighbor(train_xs,train_ys,[80]))

"""KNN에서 독립변수에 특성이 여러 개이고 특성에 따라 크기의 차이가 상당할 때"""

train_xs2= np.array([[180,0.84],[190,0.89],[120,0.32],[150,0.49],[160,0.58],[170,0.65]]) #[키,몸무게]
train_ys2= np.array([32,33,24,26,30,31]) #[허리둘레]
print(find_k_nearest_neighbor(train_xs2,train_ys2,[210,1.10],k=1))
print(find_k_nearest_neighbor(train_xs2,train_ys2,[186,0.84],k=1))

distance([180,0.84],[186,0.84]),distance([190,0.89],[186,0.84])

plt.figure(figsize=(5,5))
plt.plot(train_xs2[:,0],train_xs2[:,1],'ro',label='sample')
plt.plot([186],[0.84],'b^',label="test")
plt.legend()
plt.xlim(0,210)
plt.ylim(0,210)
plt.show()

heights = train_xs2[:,0]
hm = heights.mean()
hs = heights.std()
heights2 = heights-hm #수평 이동
heights3 = heights2/hs #표준편차만큼 비율 조절

hm3 = heights3.mean()
hs3 = heights3.std()
hm3,hs3

weights = train_xs2[:,1]
wm = weights.mean()
ws = weights.std()

weights2 = weights-wm #수평 이동
weights3 = weights2/ws #표준편차만큼 비율 조절

wm3 = weights3.mean()
ws3 = weights3.std()
wm3,ws3

rescaled_train_xs =np.array([ [heights3[i],w] for i,w in enumerate(weights3) ])
rescaled_train_xs

h = 186
w = 0.84
w3 = (w-wm)/ws
h3 = (h-hm)/hs
h3,w3

print(find_k_nearest_neighbor(rescaled_train_xs,train_ys2,[h3,w3],k=1))

plt.figure(figsize=(5,5))
plt.plot(rescaled_train_xs[:,0],rescaled_train_xs[:,1],'ro',label='sample')
plt.plot([h3],[w3],'b^',label="test")
plt.legend()
plt.xlim(-2,2)
plt.ylim(-2,2)
plt.show()

"""# 목요일에 다룰 내용들

선형 회귀 VS KNN 회귀

회귀 모델 VS 분류 모델(KNN 분류) VS 군집 모델(KMeans)
"""